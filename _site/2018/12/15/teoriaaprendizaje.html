<!DOCTYPE HTML>
<!--
Prologue by HTML5 UP
html5up.net | @ajlkn
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
Jekyll integration by Chris Bobbe | chrisbobbe.github.io
-->
<html><head><!-- Robots -->
  <meta name="robots" content="index, follow" /><link rel="canonical" href="http://localhost:4000/Topicos-IA-UNISON/2018/12/15/teoriaaprendizaje.html" /><!-- Title, description, author --><title>Cuando una máquina aprende | Tópicos Avanzados de Inteligencia Artificial</title>
  <meta name="description" content="Blog para el curso de Tópicos Avanzados de Inteligencia Artificial de la Lic. en Ciencias de la Computación de la Universidad de Sonora." />
  <meta name="author" content="Ivan Moreno" />
  
  <!-- Open Graph -->
  <meta property="og:title" content="Cuando una máquina aprende | Tópicos Avanzados de Inteligencia Artificial" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://localhost:4000/Topicos-IA-UNISON/assets/images/avatar.jpg" />
  <meta property="og:url" content="http://localhost:4000/Topicos-IA-UNISON/2018/12/15/teoriaaprendizaje.html" />
  <meta property="og:site_name" content="Tópicos Avanzados de Inteligencia Artificial" />
  <meta property="og:description" content="Blog para el curso de Tópicos Avanzados de Inteligencia Artificial de la Lic. en Ciencias de la Computación de la Universidad de Sonora." />
  
  <!-- Styles -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!--[if lte IE 8]><script src="/Topicos-IA-UNISON/assets/js/ie/html5shiv.js" defer></script><![endif]-->
  <link rel="stylesheet" href="/Topicos-IA-UNISON/assets/css/main.css" />
  <!--[if lte IE 8]><link rel="stylesheet" href="/Topicos-IA-UNISON/assets/css/ie8.css" /><![endif]-->
  <!--[if lte IE 9]><link rel="stylesheet" href="/Topicos-IA-UNISON/assets/css/ie9.css" /><![endif]-->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js" defer></script>
  <script src="/Topicos-IA-UNISON/assets/js/jquery.scrolly.min.js" defer></script>
  <script src="/Topicos-IA-UNISON/assets/js/jquery.scrollzer.min.js" defer></script>
  <script src="/Topicos-IA-UNISON/assets/js/skel.min.js" defer></script>
  <script src="/Topicos-IA-UNISON/assets/js/util.js" defer></script>
  <!--[if lte IE 8]><script src="/Topicos-IA-UNISON/assets/js/ie/respond.min.js" defer></script><![endif]-->
  <script src="/Topicos-IA-UNISON/assets/js/main.js" defer></script>

</head><body><!-- Header -->
<div id="header">
  <div class="top"><!-- Logo -->
<div id="logo">
  <a href="http://localhost:4000/Topicos-IA-UNISON/" id="home-link">
    <span class="image avatar48"><img src="/Topicos-IA-UNISON/assets/images/avatar.jpg" alt="Avatar of Ivan Moreno" /></span>
    <h1 id="title">Tópicos Avanzados de Inteligencia Artificial</h1>
    <p></p>
  </a>
</div><!-- Nav -->
<nav id="nav">
  <ul><li><a href="http://localhost:4000/Topicos-IA-UNISON/" id="inicio-link">
            <span class="icon fa-home">Inicio</span>
          </a></li><li><a href="http://localhost:4000/Topicos-IA-UNISON/blog.html" id="blog-del-curso-link">
            <span class="icon fa-pencil-alt">Blog del curso</span>
          </a></li></ul>
</nav></div>
  <div class="bottom"><!-- Social Icons -->
<ul class="icons"><li><a href="https://github.com/rexemin/Topicos-IA-UNISON" class="icon-b fa-github"><span class="label">GitHub</span></a></li><li><a href="mailto:iamorenosoto@gmail.com" class="icon fa-envelope"><span class="label">Email</span></a></li></ul>
</div>
</div>
<!-- Main -->
<div id="main">
	<!-- Post -->
	<article class="shade-two">
	  <div class="container">
			<header>
        
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
    }
    });
    </script>
    <script
        type="text/javascript"
        charset="utf-8"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
        >
        </script>
        <script
            type="text/javascript"
            charset="utf-8"
            src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
            >
            </script>


				<h2>Cuando una máquina aprende</h2>
				<p>Ivan Moreno, 15 December 2018</p>
			</header><p>Estudiamos conceptos de la teoría estadística del aprendizaje en la última parte
de la materia. Hicimos unos ejercicios para ilustrarlos.</p>

<p>Cuando queremos clasificar información a partir de una muestra de esta, generalmente
usamos aprendizaje supervisado, porque es popular. Pero hay detalles que hay que
tomar en cuenta para crear un clasificador decente. Estos detalles son estudiados
en la teoría estadística del aprendizaje (el nombre puede variar un poco).</p>

<h2 id="suposiciones">Suposiciones</h2>

<p>Todo empieza cuando tenemos un conjunto de datos $X$ (una muestra) y un conjunto
de etiquetas $Y$ que nos dicen a qué clase pertenecen, y hacemos unas
suposiciones sencillas:</p>
<ul>
  <li>Existe $ f : X \rightarrow Y $ que desconocemos</li>
  <li>$ y^i = f(x^i) + e $ donde $e$ es un error aleatorio</li>
</ul>

<p>La tarea es encontrar una hipótesis que se acerque lo suficiente a esa función
desconocida. O mejor aún, encontrar la mejor hipótesis posible. Para poder medir
el rendimiento ocupamos definir una función de pérdida $loss(h, x)$.</p>

<p>A partir de esta pérdida, tenemos dos tipos de errores: error dentro de muestra
($E_{in}$) y error fuera de muestra ($E_{out}$). Para decir que una máquina
aprende, ocupamos que $E_{in} \approx 0$ y que $E_{out} \approx E_{in}$.
Es decir, la computadora aprende a particularizar dentro del conjunto de
entrenamiento y al mismo tiempo aprende a generalizar para datos que nunca ha
visto.</p>

<p>Quizá suene fácil si simplemente conseguimos datos, los limpiamos y los
metemos en un algoritmo clásico de aprendizaje supervisado. Pero hay una cosa
llamada <strong>dimensión vc</strong> que nos indica el máximo número de datos que podemos
separar perfectamente con nuestra hipótesis. Nos da una medida de la complejidad de
nuestro modelo, y si lo complicamos demasiado y tenemos pocos datos, el algoritmo
no aprenderá. Lo mejor de esta dimensión es que si transformamos mucho nuestros
datos manualmente, incrementa, aunque no nos demos cuenta. Para ver el comportamiento
de la dimensión vc están las siguientes gráficas de una función que depende del
tamaño del conjunto de entrenamiento.</p>

<p><img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-2.png" alt="D-vc = 2" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-3.png" alt="D-vc = 3" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-5.png" alt="D-vc = 5" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-10.png" alt="D-vc = 10" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-15.png" alt="D-vc = 15" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/f_dvc-20.png" alt="D-vc = 20" /></p>

<h2 id="sesgo-varianza">Sesgo-Varianza</h2>

<p>Probablemente has pensado que no podemos tener particularización y generalización
perfectas al mismo tiempo, y es cierto, tenemos que buscar un balance entre ambas.
Además, cuando estamos probando hipótesis distintas, tenemos que encontrar un
balance entre el sesgo y la varianza.</p>

<p>El sesgo es la desviación que tiene una hipótesis de la función desconocida, o del
valor real que debería tener. La varianza es la medida de que tan distintas son
las hipótesis que tenemos.</p>

<p>Veamos un ejemplo: queremos aproximar $sen(\pi x)$ con funciones constantes y
lineales. En las siguientes imágenes podemos ver los resultados.</p>

<p><img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/sin_b.png" alt="Hipótesis constantes" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/sin_b_bias-var.png" alt="bias-var constantes" /></p>

<p>Para aproximaciones constantes tenemos poca varianza entre las hipótesis pero el
sesgo es alto, porque obviamente hay mucha diferencia entre una función constante
y una senoidal.</p>

<p><img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/sin_mb.png" alt="Hipótesis lineales" />
<img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/sin_mb_bias-var.png" alt="bias-var lineales" /></p>

<p>Para aproximaciones lineales tenemos un sesgo mucho menor que en el caso anterior,
pero por las distintas pendientes que tienen, la varianza es alta.</p>

<h2 id="ruido-en-la-muestra">Ruido en la muestra</h2>

<p>Cuando conseguimos una muestra de los datos que queremos clasificar, posiblemente
tendrá ruido. Este ruido puede ser determinista (lo que es mejor) o puede ser
estocástico.</p>

<p>En la siguiente imagen se compara el ajuste generado por un polinomio de orden 2
contra uno de orden 10 para aproximar un polinomio de Bessel de orden 3 con ruido
estocástico. Para compararlos, calculé el error en muestra de ambos de modelos y
resté el error del polinomio de orden 2 del error del polinomio de orden 10:
$ Diferencia = E_{in_{10}} - E_{in_2} $.</p>

<p>Al menos en este ejemplo un modelo más sencillo aproximó mejor en una situación con
ruido estocástico.</p>

<p><img src="https://raw.githubusercontent.com/rexemin/Topicos-IA-UNISON/master/TeoriaAprendizaje/bessel_2.png" alt="Dif besse" /></p>
</div>
	</article>
</div>
<!-- Footer -->
<div id="footer">

  <!-- Copyright -->
  <ul class="copyright">
    
      <li>Mantenido por: Ivan Moreno</li>
    
    <li>Diseño: <a href="http://html5up.net">HTML5 UP</a></li>
    <li>Integración con Jekyll: <a href="https://chrisbobbe.github.io/">Chris Bobbe</a></li>
  </ul>

</div>
</body>
</html>