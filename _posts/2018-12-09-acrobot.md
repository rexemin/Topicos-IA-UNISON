---
title: Aprendizaje por refuerzo - Parte 3 - Acrobot
author: Ivan Moreno
layout: post
mathjax: true
---

Hice que mi computadora aprendiera a controlar un acrobot para la última parte de
la unidad. Solo hacía falta usar un método, pero me emocioné y usé tres. Cada uno
con sus resultados.

Antes de pasar a todos los detalles sobre el acrobot, hay que hablar sobre el estado
de Julia. Para este ejercicio preferí usar Python 3, donde existe un único Gym y un
único Keras. En Julia, hay al menos 3 wrappers distintos para cada librería
exclusiva de Python. No todos están registrados como paquetes oficiales, y todavía
peor, no todos funcionan. Después de intentar usar dos wrappers para Gym sin éxito, mejor moví todo a Python, donde funcionó en cinco minutos con todos los requisitos que tenía.

Después de ponernos en la misma página respecto al lenguaje, es hora de ver como
usar Gym para entrenar una computadora. Aunque mi implemenntación no es la mejor
manera. No soy un experto.

Todos los scripts que usé están en
[este repositorio](https://github.com/rexemin/Topicos-IA-UNISON).

## Acrobot

El acrobot es un pequeño juguete con dos brazos y un torque en la conexión de estos.
La idea es maniobrar el torque para que el brazo inferior alcance una cierta altura,
como en la animación de abajo.

![Acrobot logrando su objetivo]({{ site.baseurl }}/img/ejemploacrobot.gif)

Para poder simular al acrobot, hace falta programar unas ecuaciones diferenciales,
pero afortunadamente, la librería Gym de OpenAI ya lo tiene cubierto. Hasta permite
grabar videos del comportamiento de los agentes.

Si tenemos una mitad hecha, falta hacer la otra mitad. Para esto, volvemos a unas
ideas anteriores:
- Políticas \$ \epsilon \$-greedy
- Discretizar el espacio de estados

Aunque la última vez dije que usaría tile-coding para entrenar, al final no lo
implementé, y mejor discreticé sencillamente el espacio de estados. Creé intervalos
del -1 al 1 donde todos los números que estuvieran en medio caerían en los mismos
lugares. Así, reduje el espacio de estados de una infinidad a la 6, a alrededor de
50000, 18000, y 12000. Que siguen siendo bastantes.

## Acrobot con Q-Learning

Para empezar, probé con algo muy sencillo: Q-Learning. En pocas líneas de código
queda el algoritmo junto con la discretización del estado. Al principio fueron
tantas entradas en el diccionario de valores-acción que se acabó la memoria
asignada a Python y el programa abortó. Así que 50000 estados sigue siendo
impensable. Luego de aumentar los intervalos, logré reducir la cardinalidad a
alrededor de 18000 estados. Lo que solucionó el problema anterior.

AGREGAR RESULTADOS Y DISCUSIÓN.

## Acrobot con doble Q-Learning

## Acrobot con Deep Q-Learning

## Lecciones
