{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando una política para jugar el 21 (con repartidor)\n",
    "\n",
    "Esta libreta utiliza dos algoritmos de programación dinámica para encontrar una política que permita jugar al 21.\n",
    "\n",
    "Las reglas para este 21 son especiales:\n",
    "- El mazo es infinito. De esta manera las 13 cartas siempre tienen las mismas probabilidades de aparecer\n",
    "- El repartidor no realiza movimientos hasta que el jugador decida parar\n",
    "- Quien llegue a 4 cartas sin pasarse automáticamente gana\n",
    "- El repartidor siempre pide una carta más si tiene 16 o menos. En otro caso, siempre para\n",
    "\n",
    "Con las reglas explicadas, definiremos una estructura para procesos de decisión markoviana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDP\n",
    "    states\n",
    "    actions\n",
    "    ρ\n",
    "    reward\n",
    "    final_s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para hallar la política\n",
    "\n",
    "### Valor de política\n",
    "\n",
    "Primero implementaremos una función que nos permitirá calcular el valor para cada estado de nuestro Proceso de Decisión de Markov con una política $\\pi$ y con factor de descuento $\\gamma$.\n",
    "\n",
    "Esta función calcula iterativamente el valor para cada estado hasta que los resultados convergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_value"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "\n",
    "Computes the value for every state in the MDP by using pol_π and discount γ.\n",
    "\"\"\"\n",
    "function policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    while !has_converged\n",
    "        has_converged = true\n",
    "    \n",
    "        for s ∈ mdp.states\n",
    "            temp = sum([mdp.ρ(s, pol_π[s], n_s) * (mdp.reward(s, pol_π[s], n_s) + γ*v[n_s]) for n_s ∈ keys(v)])\n",
    "                    \n",
    "            if temp != v[s]\n",
    "                has_converged = false\n",
    "            end\n",
    "            \n",
    "            v[s] = temp\n",
    "        end           \n",
    "    end\n",
    "            \n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de política\n",
    "\n",
    "Con la función anterior podemos programar ahora una función que encuentre una política óptima para un Proceso de Decisión de Markov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_iteration"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    policy_iteration(mdp::MDP, γ)\n",
    "\n",
    "Finds an optimal policy for the MDP using discount factor γ.\n",
    "\"\"\"\n",
    "function policy_iteration(mdp::MDP, γ)\n",
    "    pol_π = Dict(s => rand(mdp.actions) for s ∈ mdp.states)\n",
    "    \n",
    "    is_optimal = false\n",
    "    while !is_optimal\n",
    "        v = policy_value(mdp, pol_π, γ)\n",
    "        \n",
    "        is_optimal = true\n",
    "        \n",
    "        for s ∈ keys(v)\n",
    "            for a ∈ mdp.actions\n",
    "                temp = sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ*v[s]) for n_s ∈ keys(v)])\n",
    "                \n",
    "                if temp < v[s]\n",
    "                    is_optimal = false\n",
    "                    pol_π[s] = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de valor\n",
    "\n",
    "Enseguida viene una implementación de un algoritmo iterativo para encontrar una buena política, todo en un único bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter_value"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    iter_value(mdp::MDP, γ::Float64)\n",
    "\n",
    "Iteratively computes the value function of a Markov Decision Process using\n",
    "discount rate γ and then returns the optimal policy π associated with it.\n",
    "\"\"\"\n",
    "function iter_value(mdp::MDP, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    v_p = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    \n",
    "    while !has_converged\n",
    "        for s ∈ keys(v)\n",
    "            if s ∈ mdp.final_s\n",
    "                v_p[s] = mdp.reward(s, nothing, s)\n",
    "            else\n",
    "                v_p[s] = maximum([sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ * v[n_s])\n",
    "                                            for n_s ∈ mdp.states])\n",
    "                                        for a ∈ mdp.actions])\n",
    "            end\n",
    "\n",
    "            has_converged = true\n",
    "                            \n",
    "            for s ∈ keys(v)\n",
    "                if v_p[s] > v[s]\n",
    "                    v[s] = v_p[s]\n",
    "                    has_converged = false\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if has_converged\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    pol_π = Dict(s => mdp.actions[1] for s ∈ mdp.states)\n",
    "    \n",
    "    for s ∈ keys(v)\n",
    "        actions_value = Dict(a => sum([mdp.ρ(s, a, n_s) * v[n_s] for n_s ∈ mdp.states])\n",
    "                            for a ∈ mdp.actions)\n",
    "                                \n",
    "        pol_π[s] = findmax(actions_value)[2]\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo el proceso de decisión de Markov\n",
    "\n",
    "Una vez que tenemos las funciones que nos permitirán encontrar una política óptima para el 21, ahora ocupamos definir un modelo que se ajuste a estos.\n",
    "En las siguientes celdas viene el código necesario para definir un proceso de decisión de Markov que represente el juego y que sea compatible con ``policy_value``, ``policy_iteration``, ``iter_value``.\n",
    "\n",
    "### Estados del 21\n",
    "\n",
    "Primero definiremos los estados del juego como un arreglo de 5 números enteros:\n",
    "\n",
    "$$(C_p, S_p, C_d, S_d, f)$$\n",
    "\n",
    "Donde $S_p$ es la suma total de la cantidad $C_p$ de cartas que tiene el jugador. $S_d$ es la suma total de la cantidad $C_d$ de cartas que tiene la casa. $f$ nos indica si el estado es terminal o no: 0 si no lo es, 1 si lo es.\n",
    "\n",
    "Calculamos cada estado del juego de manera progresiva: primero todas las posibilidades cuando el jugador tiene 2 cartas, luego cuando tiene 3, y finalmente cuando tiene 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3304-element Array{Any,1}:\n",
       " [2, 2, 1, 1, 0]  \n",
       " [2, 3, 1, 1, 0]  \n",
       " [2, 4, 1, 1, 0]  \n",
       " [2, 5, 1, 1, 0]  \n",
       " [2, 6, 1, 1, 0]  \n",
       " [2, 7, 1, 1, 0]  \n",
       " [2, 8, 1, 1, 0]  \n",
       " [2, 9, 1, 1, 0]  \n",
       " [2, 10, 1, 1, 0] \n",
       " [2, 11, 1, 1, 0] \n",
       " [2, 12, 1, 1, 0] \n",
       " [2, 13, 1, 1, 0] \n",
       " [2, 14, 1, 1, 0] \n",
       " ⋮                \n",
       " [4, 19, 1, 11, 1]\n",
       " [4, 20, 1, 11, 1]\n",
       " [4, 21, 1, 11, 1]\n",
       " [4, 22, 1, 11, 1]\n",
       " [4, 23, 1, 11, 1]\n",
       " [4, 24, 1, 11, 1]\n",
       " [4, 25, 1, 11, 1]\n",
       " [4, 26, 1, 11, 1]\n",
       " [4, 27, 1, 11, 1]\n",
       " [4, 28, 1, 11, 1]\n",
       " [4, 29, 1, 11, 1]\n",
       " [4, 30, 1, 11, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = []\n",
    "\n",
    "# 2 cartas del jugador.\n",
    "for c ∈ 1:4\n",
    "    fin = if c < 3 10*c + 1 else 26 end\n",
    "    \n",
    "    for j ∈ c:fin\n",
    "        fin_p = if c == 1 21 else 20 end\n",
    "        \n",
    "        for i ∈ 2:fin_p\n",
    "            push!(states, [2, i, c, j, 0])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 3 cartas del jugador.\n",
    "for c ∈ 1:4\n",
    "    fin = if c < 3 10*c + 1 else 26 end\n",
    "    \n",
    "    for j ∈ c:fin\n",
    "        fin_p = if c == 1 30 else 20 end\n",
    "        \n",
    "        for i ∈ 3:fin_p\n",
    "            push!(states, [3, i, c, j, 0])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# 4 cartas del jugador.  \n",
    "for j ∈ 1:11\n",
    "    for i ∈ 4:30\n",
    "        push!(states, [4, i, 1, j, 0])\n",
    "    end\n",
    "end\n",
    "\n",
    "for s ∈ states\n",
    "    if s[2] >= 21 || s[4] >= 17 || s[1] == 4 || s[3] == 4\n",
    "        s[5] = 1\n",
    "    end\n",
    "end\n",
    "\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos revisar cuantos estados fueron generados con la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de estados:(3304,)\n"
     ]
    }
   ],
   "source": [
    "println(\"Cantidad de estados:\", size(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esos son bastantes estados, pero ni modo, podría ser mucho peor. La cantidad de estados necesarios para describir un entorno incrementa rápidamente para problemas más complejos.\n",
    "\n",
    "### Estados finales\n",
    "\n",
    "Una vez que tenemos todos los estados del 21, agregamos todos los estados finales a un único lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824-element Array{Any,1}:\n",
       " [2, 21, 1, 1, 1] \n",
       " [2, 21, 1, 2, 1] \n",
       " [2, 21, 1, 3, 1] \n",
       " [2, 21, 1, 4, 1] \n",
       " [2, 21, 1, 5, 1] \n",
       " [2, 21, 1, 6, 1] \n",
       " [2, 21, 1, 7, 1] \n",
       " [2, 21, 1, 8, 1] \n",
       " [2, 21, 1, 9, 1] \n",
       " [2, 21, 1, 10, 1]\n",
       " [2, 21, 1, 11, 1]\n",
       " [2, 2, 2, 17, 1] \n",
       " [2, 3, 2, 17, 1] \n",
       " ⋮                \n",
       " [4, 19, 1, 11, 1]\n",
       " [4, 20, 1, 11, 1]\n",
       " [4, 21, 1, 11, 1]\n",
       " [4, 22, 1, 11, 1]\n",
       " [4, 23, 1, 11, 1]\n",
       " [4, 24, 1, 11, 1]\n",
       " [4, 25, 1, 11, 1]\n",
       " [4, 26, 1, 11, 1]\n",
       " [4, 27, 1, 11, 1]\n",
       " [4, 28, 1, 11, 1]\n",
       " [4, 29, 1, 11, 1]\n",
       " [4, 30, 1, 11, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_states = []\n",
    "for s ∈ states\n",
    "    if s[5] == 1\n",
    "        push!(final_states, s)\n",
    "    end\n",
    "end\n",
    "\n",
    "final_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "\n",
    "Ahora las acciones. Esta parte es fácil, solo hay dos posibles acciones en todo momento. Esta libreta usa el vocabulario usado en los casinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{String,1}:\n",
       " \"hit\"  \n",
       " \"stand\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\"hit\", \"stand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidad de transición\n",
    "\n",
    "Seguimos con la declaración de la función que calcula la probabilidad de transición entre estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ρ (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ρ(s, a, n_s)\n",
    "    # No permitimos que se salga de un estado final.\n",
    "    if s[5] == 1\n",
    "        return 0\n",
    "    end\n",
    "    \n",
    "    if a == \"stand\"\n",
    "        # Nos ahorramos el trabajo si es una transición al mismo estado.\n",
    "        if s == n_s\n",
    "            return 1\n",
    "        end\n",
    "        \n",
    "        # No permitimos que juegue después de terminar.\n",
    "        if s[1] != n_s[1]\n",
    "            return 0\n",
    "        end\n",
    "        \n",
    "        diff_score = n_s[4] - s[4]\n",
    "        has_one_more = n_s[3] == s[3] + 1\n",
    "        same_player_score = s[2] == n_s[2]\n",
    "        \n",
    "        if has_one_more && (1 <= diff_score <= 9 || diff_score == 11) && same_player_score\n",
    "            return 1/13\n",
    "        elseif has_one_more && diff_score == 10 && same_player_score\n",
    "            return 4/13\n",
    "        end\n",
    "    else\n",
    "        # No permitimos que juegue después de terminar.\n",
    "        if s[3] != 1 || n_s[3] != 1\n",
    "            return 0\n",
    "        end\n",
    "        \n",
    "        diff_score = n_s[2] - s[2]\n",
    "        has_one_more = n_s[1] == s[1] + 1\n",
    "        same_dealer_score = s[4] == n_s[4]\n",
    "        \n",
    "        if has_one_more && (1 <= diff_score <= 9 || diff_score == 11) && same_dealer_score\n",
    "            return 1/13\n",
    "        elseif has_one_more && diff_score == 10 && same_dealer_score\n",
    "            return 4/13\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensa\n",
    "\n",
    "Completamos el modelo calculando la recompensa para cada estado. Aquí no hay ganancia ni pérdida a menos que el juego se acabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward(s, a, n_s)\n",
    "    if n_s[5] == 1\n",
    "        if n_s[2] <= 21\n",
    "            if n_s[2] == 21 || n_s[1] == 4 || n_s[2] >= n_s[4] || n_s[4] > 21\n",
    "                return 1\n",
    "            else\n",
    "                return -1\n",
    "            end\n",
    "        else\n",
    "            return -1\n",
    "        end\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el problema\n",
    "\n",
    "Como tenemos todos los preparativos listos, podemos crear un nuevo proceso de decisión de Markov que modele este 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP(Any[[2, 2, 1, 1, 0], [2, 3, 1, 1, 0], [2, 4, 1, 1, 0], [2, 5, 1, 1, 0], [2, 6, 1, 1, 0], [2, 7, 1, 1, 0], [2, 8, 1, 1, 0], [2, 9, 1, 1, 0], [2, 10, 1, 1, 0], [2, 11, 1, 1, 0]  …  [4, 21, 1, 11, 1], [4, 22, 1, 11, 1], [4, 23, 1, 11, 1], [4, 24, 1, 11, 1], [4, 25, 1, 11, 1], [4, 26, 1, 11, 1], [4, 27, 1, 11, 1], [4, 28, 1, 11, 1], [4, 29, 1, 11, 1], [4, 30, 1, 11, 1]], [\"hit\", \"stand\"], ρ, reward, Any[[2, 21, 1, 1, 1], [2, 21, 1, 2, 1], [2, 21, 1, 3, 1], [2, 21, 1, 4, 1], [2, 21, 1, 5, 1], [2, 21, 1, 6, 1], [2, 21, 1, 7, 1], [2, 21, 1, 8, 1], [2, 21, 1, 9, 1], [2, 21, 1, 10, 1]  …  [4, 21, 1, 11, 1], [4, 22, 1, 11, 1], [4, 23, 1, 11, 1], [4, 24, 1, 11, 1], [4, 25, 1, 11, 1], [4, 26, 1, 11, 1], [4, 27, 1, 11, 1], [4, 28, 1, 11, 1], [4, 29, 1, 11, 1], [4, 30, 1, 11, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_one = MDP(states, actions, ρ, reward, final_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de política\n",
    "\n",
    "Y con esto, ahora podemos probar si nuestros algoritmos realmente cumplen su tarea. Desafortunadamente, este MDP no funciona bien con las siguientes funciones. Los valores divergen. Estas pruebas se quedan en esta libreta por si te interesa depurar el modelo y mejorarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "γ = 0.9\n",
    "pol_π_1 = policy_iteration(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora revisamos el valor para la política $\\pi_1$ y los contenidos de la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "policy_value(twenty_one, pol_π_1, γ, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in states\n",
    "    println(s, \": \", pol_π_1[s])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de valor\n",
    "\n",
    "Por último, probamos el algoritmo de iteración de valor y examinamos el valor para la política $\\pi_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},String} with 3304 entries:\n",
       "  [3, 11, 4, 17, 1] => \"stand\"\n",
       "  [2, 3, 3, 6, 0]   => \"stand\"\n",
       "  [3, 6, 1, 10, 0]  => \"stand\"\n",
       "  [3, 10, 3, 26, 1] => \"stand\"\n",
       "  [2, 21, 1, 3, 1]  => \"stand\"\n",
       "  [2, 15, 1, 5, 0]  => \"stand\"\n",
       "  [4, 26, 1, 1, 1]  => \"stand\"\n",
       "  [2, 12, 1, 10, 0] => \"stand\"\n",
       "  [2, 12, 2, 15, 0] => \"stand\"\n",
       "  [4, 23, 1, 8, 1]  => \"stand\"\n",
       "  [2, 7, 3, 19, 1]  => \"stand\"\n",
       "  [3, 9, 3, 20, 1]  => \"stand\"\n",
       "  [2, 7, 2, 8, 0]   => \"stand\"\n",
       "  [2, 3, 1, 6, 0]   => \"stand\"\n",
       "  [3, 18, 1, 3, 0]  => \"stand\"\n",
       "  [2, 17, 4, 21, 1] => \"stand\"\n",
       "  [2, 4, 4, 22, 1]  => \"stand\"\n",
       "  [3, 15, 3, 10, 0] => \"stand\"\n",
       "  [2, 3, 4, 16, 1]  => \"stand\"\n",
       "  [3, 11, 2, 10, 0] => \"stand\"\n",
       "  [2, 19, 3, 15, 0] => \"stand\"\n",
       "  [3, 28, 1, 5, 1]  => \"stand\"\n",
       "  [4, 16, 1, 11, 1] => \"stand\"\n",
       "  [2, 14, 4, 4, 1]  => \"stand\"\n",
       "  [3, 12, 4, 24, 1] => \"stand\"\n",
       "  ⋮                 => ⋮"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = 0.9\n",
    "pol_π_2 = iter_value(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_value(twenty_one, pol_π_2, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states\n",
    "    println(s, \": \", pol_π_2[s])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "DDF593A59B5B4D95B15DCEE13B20C9FA",
   "lastKernelId": "aae43285-8de7-40ad-91cf-331f337c2cfd"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
