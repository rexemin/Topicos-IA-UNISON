{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando una política para jugar el 21 (sin repartidor)\n",
    "\n",
    "Esta libreta utiliza dos algoritmos de programación dinámica para encontrar una política que permita jugar al 21.\n",
    "\n",
    "Las reglas para este 21 son especiales:\n",
    "- El mazo es infinito. De esta manera las 13 cartas siempre tienen las mismas probabilidades de aparecer\n",
    "- No se toma en cuenta al repartidor\n",
    "- Si el jugador llega a 4 cartas sin pasarse automáticamente gana\n",
    "\n",
    "Con las reglas explicadas, definiremos una estructura para procesos de decisión markoviana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDP\n",
    "    states\n",
    "    actions\n",
    "    ρ\n",
    "    reward\n",
    "    final_s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para hallar la política\n",
    "\n",
    "### Valor de política\n",
    "\n",
    "Primero implementaremos una función que nos permitirá calcular el valor para cada estado de nuestro Proceso de Decisión de Markov con una política $\\pi$ y con factor de descuento $\\gamma$.\n",
    "\n",
    "Esta función calcula iterativamente el valor para cada estado hasta que los resultados convergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_value"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "\n",
    "Computes the value for every state in the MDP by using pol_π and discount γ.\n",
    "\"\"\"\n",
    "function policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    while !has_converged\n",
    "        has_converged = true\n",
    "    \n",
    "        for s ∈ keys(v)\n",
    "            temp = sum([mdp.ρ(s, pol_π[s], n_s) * (mdp.reward(s, pol_π[s], n_s) + γ*v[n_s]) for n_s ∈ keys(v)])\n",
    "                    \n",
    "            if temp != v[s]\n",
    "                has_converged = false\n",
    "            end\n",
    "            \n",
    "            v[s] = temp\n",
    "        end             \n",
    "    end\n",
    "            \n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de política\n",
    "\n",
    "Con la función anterior podemos programar ahora una función que encuentre una política óptima para un Proceso de Decisión de Markov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_iteration"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    policy_iteration(mdp::MDP, γ)\n",
    "\n",
    "Finds an optimal policy for the MDP using discount factor γ.\n",
    "\"\"\"\n",
    "function policy_iteration(mdp::MDP, γ)\n",
    "    pol_π = Dict(s => rand(mdp.actions) for s ∈ mdp.states)\n",
    "    \n",
    "    is_optimal = false\n",
    "    while !is_optimal\n",
    "        v = policy_value(mdp, pol_π, γ)\n",
    "        \n",
    "        is_optimal = true\n",
    "        \n",
    "        for s ∈ keys(v)\n",
    "            for a ∈ mdp.actions\n",
    "                temp = sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ*v[s]) for n_s ∈ keys(v)])\n",
    "                \n",
    "                if temp < v[s]\n",
    "                    is_optimal = false\n",
    "                    pol_π[s] = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de valor\n",
    "\n",
    "Enseguida viene una implementación de un algoritmo iterativo para encontrar una buena política, todo en un único bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter_value"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    iter_value(mdp::MDP, γ::Float64)\n",
    "\n",
    "Iteratively computes the value function of a Markov Decision Process using\n",
    "discount rate γ and then returns the optimal policy π associated with it.\n",
    "\"\"\"\n",
    "function iter_value(mdp::MDP, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    v_p = Dict(s => 0.0 for s ∈ mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    \n",
    "    while !has_converged\n",
    "        for s ∈ keys(v)\n",
    "            if s ∈ mdp.final_s\n",
    "                v_p[s] = mdp.reward(s, nothing, s)\n",
    "            else\n",
    "                v_p[s] = maximum([sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ * v[n_s])\n",
    "                                            for n_s ∈ mdp.states])\n",
    "                                        for a ∈ mdp.actions])\n",
    "            end\n",
    "\n",
    "            has_converged = true\n",
    "                            \n",
    "            for s ∈ keys(v)\n",
    "                if v_p[s] > v[s]\n",
    "                    v[s] = v_p[s]\n",
    "                    has_converged = false\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if has_converged\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    pol_π = Dict(s => mdp.actions[1] for s ∈ mdp.states)\n",
    "    \n",
    "    for s ∈ keys(v)\n",
    "        actions_value = Dict(a => sum([mdp.ρ(s, a, n_s) * v[n_s] for n_s ∈ mdp.states])\n",
    "                            for a ∈ mdp.actions)\n",
    "                                \n",
    "        pol_π[s] = findmax(actions_value)[2]\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo el proceso de decisión de Markov\n",
    "\n",
    "Una vez que tenemos las funciones que nos permitirán encontrar una política óptima para el 21, ahora ocupamos definir un modelo que se ajuste a estos.\n",
    "En las siguientes celdas viene el código necesario para definir un proceso de decisión de Markov que represente el juego y que sea compatible con ``policy_value``, ``policy_iteration``, ``iter_value``.\n",
    "\n",
    "### Estados del 21\n",
    "\n",
    "Primero definiremos los estados del juego como un arreglo de dos de números enteros.\n",
    "\n",
    "$$(S, C)$$\n",
    "\n",
    "Donde $S$ es la suma total de la cantidad $C$ de cartas que tiene el jugador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111-element Array{Array{Int64,1},1}:\n",
       " [2, 2] \n",
       " [2, 3] \n",
       " [2, 4] \n",
       " [2, 5] \n",
       " [2, 6] \n",
       " [2, 7] \n",
       " [2, 8] \n",
       " [2, 9] \n",
       " [2, 10]\n",
       " [2, 11]\n",
       " [2, 12]\n",
       " [2, 13]\n",
       " [2, 14]\n",
       " ⋮      \n",
       " [4, 41]\n",
       " [4, 42]\n",
       " [4, 43]\n",
       " [4, 44]\n",
       " [4, 45]\n",
       " [4, 46]\n",
       " [4, 47]\n",
       " [4, 48]\n",
       " [4, 49]\n",
       " [4, 50]\n",
       " [4, 51]\n",
       " [4, 52]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = [[2, i] for i in 2:26]\n",
    "\n",
    "for i in 3:39\n",
    "    push!(states, [3, i])\n",
    "end\n",
    "\n",
    "for i in 4:52\n",
    "    push!(states, [4, i])\n",
    "end\n",
    "\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos revisar cuantos estados fueron generados con la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de estados:(111,)\n"
     ]
    }
   ],
   "source": [
    "println(\"Cantidad de estados:\", size(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estados finales\n",
    "\n",
    "Una vez que tenemos todos los estados del 21, agregamos todos los estados finales a un único lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74-element Array{Any,1}:\n",
       " [2, 21]\n",
       " [2, 22]\n",
       " [2, 23]\n",
       " [2, 24]\n",
       " [2, 25]\n",
       " [2, 26]\n",
       " [3, 21]\n",
       " [3, 22]\n",
       " [3, 23]\n",
       " [3, 24]\n",
       " [3, 25]\n",
       " [3, 26]\n",
       " [3, 27]\n",
       " ⋮      \n",
       " [4, 41]\n",
       " [4, 42]\n",
       " [4, 43]\n",
       " [4, 44]\n",
       " [4, 45]\n",
       " [4, 46]\n",
       " [4, 47]\n",
       " [4, 48]\n",
       " [4, 49]\n",
       " [4, 50]\n",
       " [4, 51]\n",
       " [4, 52]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_states = []\n",
    "for s ∈ states\n",
    "    if s[1] == 4 || s[2] >= 21\n",
    "        push!(final_states, s)\n",
    "    end\n",
    "end\n",
    "\n",
    "final_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "\n",
    "Ahora las acciones. Esta parte es fácil, solo hay dos posibles acciones en todo momento. Esta libreta usa el vocabulario usado en los casinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{String,1}:\n",
       " \"hit\"  \n",
       " \"stand\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\"hit\", \"stand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidad de transición\n",
    "\n",
    "Seguimos con la declaración de la función que calcula la probabilidad de transición entre estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ρ (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ρ(s, a, n_s)\n",
    "    if a == \"stand\"\n",
    "        if s == n_s\n",
    "            return 1\n",
    "        else\n",
    "            return 0\n",
    "        end\n",
    "    else\n",
    "        diff_score = n_s[2] - s[2]\n",
    "        \n",
    "        if n_s[1] == s[1] + 1 && diff_score >= 1 && diff_score <= 13 \n",
    "            return 1/13\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensa\n",
    "\n",
    "Completamos el modelo calculando la recompensa para cada estado. Aquí no hay ganancia ni pérdida a menos que el juego se acabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward(s, a, n_s)\n",
    "    if (n_s[1] == 4 && n_s[2] <= 21) || n_s[2] == 21\n",
    "        return 1\n",
    "    elseif n_s[2] > 21\n",
    "        return -1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el problema\n",
    "\n",
    "Como tenemos todos los preparativos listos, podemos crear un nuevo proceso de decisión de Markov que modele este 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP(Array{Int64,1}[[2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11]  …  [4, 43], [4, 44], [4, 45], [4, 46], [4, 47], [4, 48], [4, 49], [4, 50], [4, 51], [4, 52]], [\"hit\", \"stand\"], ρ, reward, Any[[2, 21], [2, 22], [2, 23], [2, 24], [2, 25], [2, 26], [3, 21], [3, 22], [3, 23], [3, 24]  …  [4, 43], [4, 44], [4, 45], [4, 46], [4, 47], [4, 48], [4, 49], [4, 50], [4, 51], [4, 52]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_one = MDP(states, actions, ρ, reward, final_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de política\n",
    "\n",
    "Y con esto, ahora podemos probar si nuestros algoritmos realmente cumplen su tarea. Primero probamos el algoritmo de iteración de políticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},String} with 111 entries:\n",
       "  [3, 3]  => \"stand\"\n",
       "  [4, 4]  => \"hit\"\n",
       "  [4, 16] => \"hit\"\n",
       "  [3, 25] => \"stand\"\n",
       "  [3, 31] => \"stand\"\n",
       "  [3, 29] => \"stand\"\n",
       "  [3, 4]  => \"stand\"\n",
       "  [4, 8]  => \"hit\"\n",
       "  [4, 43] => \"stand\"\n",
       "  [4, 52] => \"stand\"\n",
       "  [2, 2]  => \"stand\"\n",
       "  [4, 19] => \"hit\"\n",
       "  [4, 51] => \"stand\"\n",
       "  [3, 17] => \"hit\"\n",
       "  [4, 48] => \"stand\"\n",
       "  [3, 27] => \"stand\"\n",
       "  [2, 14] => \"hit\"\n",
       "  [4, 22] => \"stand\"\n",
       "  [4, 36] => \"stand\"\n",
       "  [2, 17] => \"hit\"\n",
       "  [3, 33] => \"stand\"\n",
       "  [4, 27] => \"stand\"\n",
       "  [2, 24] => \"stand\"\n",
       "  [3, 26] => \"stand\"\n",
       "  [4, 38] => \"stand\"\n",
       "  ⋮       => ⋮"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = 0.9\n",
    "pol_π_1 = policy_iteration(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora revisamos el valor para la política $\\pi_1$ y los contenidos de la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},Float64} with 111 entries:\n",
       "  [3, 3]  => 0.0\n",
       "  [4, 4]  => 0.0\n",
       "  [4, 16] => 0.0\n",
       "  [3, 25] => -10.0\n",
       "  [3, 31] => -10.0\n",
       "  [3, 29] => -10.0\n",
       "  [3, 4]  => 0.0\n",
       "  [4, 8]  => 0.0\n",
       "  [4, 43] => -10.0\n",
       "  [4, 52] => -10.0\n",
       "  [2, 2]  => 0.0\n",
       "  [4, 19] => 0.0\n",
       "  [4, 51] => -10.0\n",
       "  [3, 17] => -6.61538\n",
       "  [4, 48] => -10.0\n",
       "  [3, 27] => -10.0\n",
       "  [2, 14] => -8.15444\n",
       "  [4, 22] => -10.0\n",
       "  [4, 36] => -10.0\n",
       "  [2, 17] => -9.26391\n",
       "  [3, 33] => -10.0\n",
       "  [4, 27] => -10.0\n",
       "  [2, 24] => -10.0\n",
       "  [3, 26] => -10.0\n",
       "  [4, 38] => -10.0\n",
       "  ⋮       => ⋮"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_value(twenty_one, pol_π_1, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in states\n",
    "    println(s, \": \", pol_π_1[s])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteración de valor\n",
    "\n",
    "Por último, probamos el algoritmo de iteración de valor y examinamos el valor para la política $\\pi_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},String} with 111 entries:\n",
       "  [3, 3]  => \"stand\"\n",
       "  [4, 4]  => \"stand\"\n",
       "  [4, 16] => \"stand\"\n",
       "  [3, 25] => \"stand\"\n",
       "  [3, 31] => \"stand\"\n",
       "  [3, 29] => \"stand\"\n",
       "  [3, 4]  => \"hit\"\n",
       "  [4, 8]  => \"stand\"\n",
       "  [4, 43] => \"stand\"\n",
       "  [4, 52] => \"stand\"\n",
       "  [2, 2]  => \"hit\"\n",
       "  [4, 19] => \"stand\"\n",
       "  [4, 51] => \"stand\"\n",
       "  [3, 17] => \"stand\"\n",
       "  [4, 48] => \"stand\"\n",
       "  [3, 27] => \"stand\"\n",
       "  [2, 14] => \"stand\"\n",
       "  [4, 22] => \"stand\"\n",
       "  [4, 36] => \"stand\"\n",
       "  [2, 17] => \"stand\"\n",
       "  [3, 33] => \"stand\"\n",
       "  [4, 27] => \"stand\"\n",
       "  [2, 24] => \"stand\"\n",
       "  [3, 26] => \"stand\"\n",
       "  [4, 38] => \"stand\"\n",
       "  ⋮       => ⋮"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = 0.9\n",
    "pol_π_2 = iter_value(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},Float64} with 111 entries:\n",
       "  [3, 3]  => 0.0\n",
       "  [4, 4]  => 1.0\n",
       "  [4, 16] => 1.0\n",
       "  [3, 25] => -1.0\n",
       "  [3, 31] => -1.0\n",
       "  [3, 29] => -1.0\n",
       "  [3, 4]  => 1.9\n",
       "  [4, 8]  => 1.0\n",
       "  [4, 43] => -1.0\n",
       "  [4, 52] => -1.0\n",
       "  [2, 2]  => 1.01183\n",
       "  [4, 19] => 1.0\n",
       "  [4, 51] => -1.0\n",
       "  [3, 17] => 0.0\n",
       "  [4, 48] => -1.0\n",
       "  [3, 27] => -1.0\n",
       "  [2, 14] => 0.0\n",
       "  [4, 22] => -1.0\n",
       "  [4, 36] => -1.0\n",
       "  [2, 17] => 0.0\n",
       "  [3, 33] => -1.0\n",
       "  [4, 27] => -1.0\n",
       "  [2, 24] => -1.0\n",
       "  [3, 26] => -1.0\n",
       "  [4, 38] => -1.0\n",
       "  ⋮       => ⋮"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_value(twenty_one, pol_π_2, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states\n",
    "    println(s, \": \", pol_π_2[s])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
