{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando una política para jugar el 21\n",
    "\n",
    "Esta libreta utiliza un algoritmo de programación dinámica para encontrar una política que permita jugar al 21.\n",
    "\n",
    "Las reglas para este 21 son especiales:\n",
    "- El mazo es infinito. De esta manera las 13 cartas siempre tienen las mismas probabilidades de aparecer\n",
    "- El jugador realiza dos acciones antes de que el repartidor juegue\n",
    "- Quien llegue a 4 cartas sin pasarse automáticamente gana\n",
    "\n",
    "Con las reglas explicadas, definiremos una estructura para procesos de decisión markoviana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDP\n",
    "    states\n",
    "    actions\n",
    "    ρ\n",
    "    reward\n",
    "    final_s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para hallar la política\n",
    "\n",
    "Enseguida viene una implementación de un algoritmo iterativo para encontrar una buena política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter_value (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    iter_value(mdp::MDP, γ::Float64)\n",
    "\n",
    "Iteratively computes the value function of a Markov Decision Process using\n",
    "discount rate γ and then returns the optimal policy π associated with it.\n",
    "\"\"\"\n",
    "function iter_value(mdp::MDP, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s in mdp.states)\n",
    "    v_p = Dict(s => 0.0 for s in mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    \n",
    "    while !has_converged\n",
    "        for s in keys(v)\n",
    "            v_p[s] = maximum([sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ * v[n_s])\n",
    "                                        for n_s in mdp.states])\n",
    "                                    for a in mdp.actions])\n",
    "\n",
    "            has_converged = true\n",
    "                            \n",
    "            for s in keys(v)\n",
    "                if v_p[s] > v[s]\n",
    "                    v[s] = v_p[s]\n",
    "                    has_converged = false\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if has_converged\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    pol_π = Dict(s => \"\" for s in mdp.states)\n",
    "    \n",
    "    for s in keys(v)\n",
    "        actions_value = Dict(a => sum([mdp.ρ(s, a, n_s) * v[n_s] for n_s in mdp.states])\n",
    "                            for a in mdp.actions)\n",
    "                                \n",
    "        pol_π[s] = findmax(actions_value)[2]\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo el proceso de decisión de Markov\n",
    "\n",
    "En las siguientes celdas viene el código necesario para definir un proceso de decisión de Markov que represente el juego y que sea compatible con ``iter_value``.\n",
    "\n",
    "Primero definiremos los estados del juego como una 2-tupla de tuplas (de hasta 4 elementos cada una) de números enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [[2, i] for i in 2:26]\n",
    "\n",
    "for i in 3:39\n",
    "    push!(states, [3, i])\n",
    "end\n",
    "\n",
    "for i in 4:52\n",
    "    push!(states, [4, i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora las acciones. Esta parte es fácil, solo hay dos posibles acciones en todo momento. Esta libreta usa el vocabulario usado en los casinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{String,1}:\n",
       " \"hit\"  \n",
       " \"stand\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\"hit\", \"stand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con la declaración de la función que calcule la probabilidad de transición entre estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ρ (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ρ(s, a, n_s)\n",
    "    if a == \"stand\"\n",
    "        if s == n_s\n",
    "            return 1\n",
    "        else\n",
    "            return 0\n",
    "        end\n",
    "    else\n",
    "        diff_score = n_s[2] - s[2]\n",
    "        \n",
    "        if n_s[1] == s[1] + 1 && diff_score >= 1 && diff_score <= 13 \n",
    "            return 1/13\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casi terminando, calculamos la recompensa para cada estado. Aquí no hay ganancia ni pérdida a menos que el juego se acabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward(s, a, n_s)\n",
    "    if (n_s[1] == 4 && n_s[2] <= 21) || n_s[2] == 21\n",
    "        return 1\n",
    "    elseif n_s[2] > 21\n",
    "        return -1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el problema\n",
    "\n",
    "Como tenemos todos los preparativos listos, podemos crear un nuevo proceso de decisión de Markov que modele este 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP(Array{Int64,1}[[2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11]  …  [4, 43], [4, 44], [4, 45], [4, 46], [4, 47], [4, 48], [4, 49], [4, 50], [4, 51], [4, 52]], [\"hit\", \"stand\"], ρ, reward, Any[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_one = MDP(states, actions, ρ, reward, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con esto, ahora podemos probar si nuestro algoritmo realmente cumple su tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Int64,1},String} with 111 entries:\n",
       "  [3, 3]  => \"stand\"\n",
       "  [4, 4]  => \"stand\"\n",
       "  [4, 16] => \"stand\"\n",
       "  [3, 25] => \"stand\"\n",
       "  [3, 31] => \"stand\"\n",
       "  [3, 29] => \"stand\"\n",
       "  [3, 4]  => \"hit\"\n",
       "  [4, 8]  => \"stand\"\n",
       "  [4, 43] => \"stand\"\n",
       "  [4, 52] => \"stand\"\n",
       "  [2, 2]  => \"hit\"\n",
       "  [4, 19] => \"stand\"\n",
       "  [4, 51] => \"stand\"\n",
       "  [3, 17] => \"stand\"\n",
       "  [4, 48] => \"stand\"\n",
       "  [3, 27] => \"stand\"\n",
       "  [2, 14] => \"stand\"\n",
       "  [4, 22] => \"stand\"\n",
       "  [4, 36] => \"stand\"\n",
       "  [2, 17] => \"stand\"\n",
       "  [3, 33] => \"stand\"\n",
       "  [4, 27] => \"stand\"\n",
       "  [2, 24] => \"stand\"\n",
       "  [3, 26] => \"stand\"\n",
       "  [4, 38] => \"stand\"\n",
       "  ⋮       => ⋮"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ = 0.8\n",
    "pol_π = iter_value(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados. No muy buenos por ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]hit\n",
      "[2, 3]stand\n",
      "[2, 4]stand\n",
      "[2, 5]stand\n",
      "[2, 6]stand\n",
      "[2, 7]stand\n",
      "[2, 8]stand\n",
      "[2, 9]stand\n",
      "[2, 10]stand\n",
      "[2, 11]stand\n",
      "[2, 12]stand\n",
      "[2, 13]stand\n",
      "[2, 14]stand\n",
      "[2, 15]stand\n",
      "[2, 16]stand\n",
      "[2, 17]stand\n",
      "[2, 18]stand\n",
      "[2, 19]stand\n",
      "[2, 20]stand\n",
      "[2, 21]stand\n",
      "[2, 22]stand\n",
      "[2, 23]stand\n",
      "[2, 24]stand\n",
      "[2, 25]stand\n",
      "[2, 26]stand\n",
      "[3, 3]stand\n",
      "[3, 4]hit\n",
      "[3, 5]hit\n",
      "[3, 6]hit\n",
      "[3, 7]hit\n",
      "[3, 8]hit\n",
      "[3, 9]hit\n",
      "[3, 10]hit\n",
      "[3, 11]hit\n",
      "[3, 12]hit\n",
      "[3, 13]hit\n",
      "[3, 14]hit\n",
      "[3, 15]hit\n",
      "[3, 16]stand\n",
      "[3, 17]stand\n",
      "[3, 18]stand\n",
      "[3, 19]stand\n",
      "[3, 20]stand\n",
      "[3, 21]stand\n",
      "[3, 22]stand\n",
      "[3, 23]stand\n",
      "[3, 24]stand\n",
      "[3, 25]stand\n",
      "[3, 26]stand\n",
      "[3, 27]stand\n",
      "[3, 28]stand\n",
      "[3, 29]stand\n",
      "[3, 30]stand\n",
      "[3, 31]stand\n",
      "[3, 32]stand\n",
      "[3, 33]stand\n",
      "[3, 34]stand\n",
      "[3, 35]stand\n",
      "[3, 36]stand\n",
      "[3, 37]stand\n",
      "[3, 38]stand\n",
      "[3, 39]stand\n",
      "[4, 4]stand\n",
      "[4, 5]stand\n",
      "[4, 6]stand\n",
      "[4, 7]stand\n",
      "[4, 8]stand\n",
      "[4, 9]stand\n",
      "[4, 10]stand\n",
      "[4, 11]stand\n",
      "[4, 12]stand\n",
      "[4, 13]stand\n",
      "[4, 14]stand\n",
      "[4, 15]stand\n",
      "[4, 16]stand\n",
      "[4, 17]stand\n",
      "[4, 18]stand\n",
      "[4, 19]stand\n",
      "[4, 20]stand\n",
      "[4, 21]stand\n",
      "[4, 22]stand\n",
      "[4, 23]stand\n",
      "[4, 24]stand\n",
      "[4, 25]stand\n",
      "[4, 26]stand\n",
      "[4, 27]stand\n",
      "[4, 28]stand\n",
      "[4, 29]stand\n",
      "[4, 30]stand\n",
      "[4, 31]stand\n",
      "[4, 32]stand\n",
      "[4, 33]stand\n",
      "[4, 34]stand\n",
      "[4, 35]stand\n",
      "[4, 36]stand\n",
      "[4, 37]stand\n",
      "[4, 38]stand\n",
      "[4, 39]stand\n",
      "[4, 40]stand\n",
      "[4, 41]stand\n",
      "[4, 42]stand\n",
      "[4, 43]stand\n",
      "[4, 44]stand\n",
      "[4, 45]stand\n",
      "[4, 46]stand\n",
      "[4, 47]stand\n",
      "[4, 48]stand\n",
      "[4, 49]stand\n",
      "[4, 50]stand\n",
      "[4, 51]stand\n",
      "[4, 52]stand\n"
     ]
    }
   ],
   "source": [
    "for s in states\n",
    "    println(s, pol_π[s])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
