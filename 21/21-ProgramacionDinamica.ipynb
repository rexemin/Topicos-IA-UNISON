{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando una política para jugar el 21\n",
    "\n",
    "Esta libreta utiliza un algoritmo de programación dinámica para encontrar una política que permita jugar al 21.\n",
    "\n",
    "Las reglas para este 21 son especiales:\n",
    "- El mazo es infinito. De esta manera las 13 cartas siempre tienen las mismas probabilidades de aparecer\n",
    "- El jugador realiza dos acciones antes de que el repartidor juegue\n",
    "- Quien llegue a 4 cartas sin pasarse automáticamente gana\n",
    "\n",
    "Con las reglas explicadas, definiremos una estructura para procesos de decisión markoviana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDP\n",
    "    states\n",
    "    actions\n",
    "    ρ\n",
    "    reward\n",
    "    final_s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para hallar la política\n",
    "\n",
    "Primero implementaremos una función que nos permitirá calcular el valor para cada estado de nuestro Proceso de Decisión de Markov con una política $\\pi$ y con factor de descuento $\\gamma$.\n",
    "\n",
    "Esta función calcula iterativamente el valor para cada estado hasta que los resultados convergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "\n",
    "Computes the value for every state in the MDP by using pol_π and discount γ.\n",
    "\"\"\"\n",
    "function policy_value(mdp::MDP, pol_π::Dict, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s in mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    while !has_converged\n",
    "        has_converged = true\n",
    "    \n",
    "        for s in keys(v)\n",
    "            temp = sum([mdp.ρ(s, pol_π[s], n_s) * (mdp.reward(s, pol_π[s], n_s) + γ*v[n_s]) for n_s in keys(v)])\n",
    "                    \n",
    "            if temp != v[s]\n",
    "                has_converged = false\n",
    "            end\n",
    "            \n",
    "            v[s] = temp\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    return v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función anterior podemos programar ahora una función que encuentre una política óptima para un Proceso de Decisión de Markov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    policy_iteration(mdp::MDP, γ)\n",
    "\n",
    "Finds an optimal policy for the MDP using discount factor γ.\n",
    "\"\"\"\n",
    "function policy_iteration(mdp::MDP, γ)\n",
    "    pol_π = Dict(s => sample(mdp.actions) for s in mdp.states)\n",
    "    \n",
    "    is_optimal = false\n",
    "    while !is_optimal\n",
    "        v = policy_value(mdp, pol_π, γ)\n",
    "        \n",
    "        is_optimal = true\n",
    "        \n",
    "        for s in keys(v)\n",
    "            for a in mdp.actions\n",
    "                temp = sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ*v[s]) for n_s in keys(v)])\n",
    "                \n",
    "                if temp < v[s]\n",
    "                    is_optimal = false\n",
    "                    pol_π[s] = a\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enseguida viene una implementación de un algoritmo iterativo para encontrar una buena política, todo en un único bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    iter_value(mdp::MDP, γ::Float64)\n",
    "\n",
    "Iteratively computes the value function of a Markov Decision Process using\n",
    "discount rate γ and then returns the optimal policy π associated with it.\n",
    "\"\"\"\n",
    "function iter_value(mdp::MDP, γ::Float64)\n",
    "    v = Dict(s => 0.0 for s in mdp.states)\n",
    "    v_p = Dict(s => 0.0 for s in mdp.states)\n",
    "    \n",
    "    has_converged = false\n",
    "    \n",
    "    while !has_converged\n",
    "        for s in keys(v)\n",
    "            v_p[s] = maximum([sum([mdp.ρ(s, a, n_s) * (mdp.reward(s, a, n_s) + γ * v[n_s])\n",
    "                                        for n_s in mdp.states])\n",
    "                                    for a in mdp.actions])\n",
    "\n",
    "            has_converged = true\n",
    "                            \n",
    "            for s in keys(v)\n",
    "                if v_p[s] > v[s]\n",
    "                    v[s] = v_p[s]\n",
    "                    has_converged = false\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if has_converged\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    pol_π = Dict(s => \"\" for s in mdp.states)\n",
    "    \n",
    "    for s in keys(v)\n",
    "        actions_value = Dict(a => sum([mdp.ρ(s, a, n_s) * v[n_s] for n_s in mdp.states])\n",
    "                            for a in mdp.actions)\n",
    "                                \n",
    "        pol_π[s] = findmax(actions_value)[2]\n",
    "    end\n",
    "    \n",
    "    return pol_π\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo el proceso de decisión de Markov\n",
    "\n",
    "En las siguientes celdas viene el código necesario para definir un proceso de decisión de Markov que represente el juego y que sea compatible con ``policy_value``, ``policy_iteration``, ``iter_value``.\n",
    "\n",
    "Primero definiremos los estados del juego como un arreglo de dos de números enteros.\n",
    "\n",
    "$$(S, C)$$\n",
    "\n",
    "Donde $S$ es la suma total de la cantidad $C$ de cartas que tiene el jugador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [[2, i] for i in 2:26]\n",
    "\n",
    "for i in 3:39\n",
    "    push!(states, [3, i])\n",
    "end\n",
    "\n",
    "for i in 4:52\n",
    "    push!(states, [4, i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora las acciones. Esta parte es fácil, solo hay dos posibles acciones en todo momento. Esta libreta usa el vocabulario usado en los casinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"hit\", \"stand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con la declaración de la función que calcula la probabilidad de transición entre estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ρ(s, a, n_s)\n",
    "    if a == \"stand\"\n",
    "        if s == n_s\n",
    "            return 1\n",
    "        else\n",
    "            return 0\n",
    "        end\n",
    "    else\n",
    "        diff_score = n_s[2] - s[2]\n",
    "        \n",
    "        if n_s[1] == s[1] + 1 && diff_score >= 1 && diff_score <= 13 \n",
    "            return 1/13\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casi terminando, calculamos la recompensa para cada estado. Aquí no hay ganancia ni pérdida a menos que el juego se acabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reward(s, a, n_s)\n",
    "    if (n_s[1] == 4 && n_s[2] <= 21) || n_s[2] == 21\n",
    "        return 1\n",
    "    elseif n_s[2] > 21\n",
    "        return -1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el problema\n",
    "\n",
    "Como tenemos todos los preparativos listos, podemos crear un nuevo proceso de decisión de Markov que modele este 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_one = MDP(states, actions, ρ, reward, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con esto, ahora podemos probar si nuestro algoritmo realmente cumple su tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.8\n",
    "pol_π = policy_iteration(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0.8\n",
    "pol_π = iter_value(twenty_one, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_value(twenty_one, pol_π, γ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los resultados. No muy buenos por ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in states\n",
    "    println(s, pol_π[s])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
